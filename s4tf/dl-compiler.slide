Look foward to Swift for TensorFlow

Yi Wang
Principal Engineer
Ant Financial
yi.w@antfin.com


* Overview

Paddle: Towards deep learning compiler

- history, adoptation, and business impact

Outlook: Why we need deep learning compilers

- Inference and training are Inseparable
- Imitation learning and autonomous driving
- Distributed training and elastic scheduling
- Federated learning

Discusions: Want to learn from you!


* The Presenter

- 2007 ~ 2010: Google China. Researcher.
- 2010 ~ 2014: Tencent. Engineering Director of Advertising. 
- 2014: LinkedIn. Senior Staff Data Scientist.
- 2015: Scaled Inference. Head of Research.
- 2016 ~ 2018: Baidu Silicon Valley Research. Principal Engineer.
- 2018 ~ present: Ant Financial.  Principal Engineer.

.image deepnav-small.png _ 500



* Paddle: The History

- 2012: Wei Xu wrote the initial version; graph-based, supports RNN.
- 2013: The CEO's Award due to contributions to Baidu products.
- 2016: Open sourced.  I led the technology upgrading.
- 2017: Released Paddle Fluid, the deep learning VM/compiler.
- 2018: 90% Baidu products turned to use Fluid, including Web search and online advertising.


* Paddle: The Community

.image community.png _ 900

Left: when graph-based Paddle was opened sourced. Right: half-year after the release of Paddle Fluid.


* Paddle: Towards a Compiler

- IR as protobuf messages: https://github.com/PaddlePaddle/Paddle/pull/3322
- The IR design: https://github.com/PaddlePaddle/Paddle/pull/4241
- The compiler design: https://github.com/PaddlePaddle/Paddle/pull/7178
- The CSP design: https://github.com/PaddlePaddle/Paddle/pull/6394


* Paddle: Intermediate Representation

- Has *block*, function *signature*, function *call*, and [[https://github.com/PaddlePaddle/Paddle/pull/4241][*program*]].
- Need [[https://github.com/PaddlePaddle/Paddle/issues/10244][user-defined functions]].

.image ir.png _ 700


* Paddle: The VM

The [[https://github.com/PaddlePaddle/Paddle/pull/4537][executor]] works like an VM, which runs a `Program` protobuf message.

.image interpreter.png _ 800


* Paddle: Compiler Backends as Transpilers

- For example, the [[https://github.com/wangkuiyi/Paddle/blob/8909708c1a91d580ed3c8cb12d1cab61338eac39/doc/design/fluid_compiler.md][CUDA backend]] converts `Program` messages into `.cu` files.

.image compiler.png _ 700


* Paddle: Scope Hiearchy

The compiler/VM needs to track variable values for the backward pass, thus cannot pop stack frames. This generalizes the stack into a forest, or a [[https://github.com/PaddlePaddle/Paddle/pull/3116][scope hiearchy]].

.image scope.png _ 800


* Paddle: Transpilers

- The [[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/transpiler/distribute_transpiler.py][distributed-training transpiler]] converts a `Program` message into a trainer program and a parameter server program.

- The [[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/transpiler/inference_transpiler.py][training-to-inference transpiler]] extracts a single iteration of inference code out from a training `Program`, so other transpilers can generate TensorFlow/Caffe graphs.

- The [[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/transpiler/memory_optimization_transpiler.py][early-free memory transpiler]] detects tensors that can be freed early and inserts `FreeTensorOperators` into the right locations of the program.

The [[https://en.wikipedia.org/wiki/Unix_philosophy][Unix philosophy]] -- make each program do one thing well.


* Paddle: The Frontend Language

We are too lazy to re-implement the compiler frontend for a programming language; so we slightly customize Python to generate the `Program` protobuf message.

Take [[https://github.com/PaddlePaddle/Paddle/issues/3119][IfElse]] as an example:

.image ifelse.png _ 700



* Outlook: Inference and Training are Inseparable

- Search/ads/recommendation are the money printers of the Internet industry.
- Users' short-term interests lead to most clicks.
- To capture the short-term interests, we want online learning of ranking models.
- The training data is the session log stream.  The inference input is online queries.
- Good to make the modeling system a RPC server, which provides online learning calls and online inference calls.



* Outlook: Inference and Training are Inseparable

A model/application perspective:

- computer vision -- face recognition
- NLP -- speech recognition
- GAN -- query rewrite, keyword generation from landing page
- reinforcement learning -- AlphaGo
- imitation learning -- autonomous driving

GAN, reinforcement learning, imitation learning:

- Part of the training data comes from running inference.
- impossbile to deploy a "trained" model for inference.




* Discussion: Breakthrough

- Take *tape* and *graph-of-operators* as two extremes of usability and performance.
- PyTorch 1.0 tracing model is a compromise in between.
- A compiler could achieve both.


         Usability
             ^ 
             | PyTorch                      compiler
             o..............................o
             |                              :
             |       o tracing mode         :
             |              o scripting mode:
             |                              :
             |------------------------------o--> Performance
                                        TensorFlow
                             

* Discussion: Usability

- Imperative programming
- Syntax highlight and IDE integration
- Debugger
- Source-to-source autodiff reveals details (Will Swift4TF do this?)


* Discussion: Performance

Tape-based v.s. compiler-based.

- The tracing mode is like JIT-compile. We have to balance runtime cost of optimization and gain.
- Compilers move optimizations ahead of runtime, free to do all optimizations.

Remove the dependency to runtime-information.

- `cudnn` includes multiple implementations of CNN, and chooses one at runtime considering GPU model and filter size.
- Maybe [[https://en.wikipedia.org/wiki/Universal_binary][universal binary]] -- let the compiler generate multiple choices automatically?




