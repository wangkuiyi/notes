Lessons from Paddle
Towards Deep Learning Compiler and Its Applications

Yi Wang
Principal Engineer
Ant Financial
yi.w@antfin.com


* Overview

- Paddle: Baidu's open-source DL system

- Requirements and Lessons: what we learned from developing and deploying Paddle

- Discussions: what we want to learn from you


* The Presenter

- 2007 ~ 2010: Google China. Researcher.
- 2010 ~ 2014: Tencent. Engineering Director of Advertising. 
- 2014: LinkedIn. Senior Staff Data Scientist.
- 2015: Scaled Inference. Head of Research.
- 2016 ~ 2018: Baidu Silicon Valley Research. Principal Engineer.
- 2018 ~ present: Ant Financial.  Principal Engineer.

.image deeepnav-small.png _ 500


* Autonomous Driving


* Autonomous Driving: To Learn the Future

- Caffe can do CNN but not RNN.
- TensorFlow does RNN well, but GAN code is a little bit lengthy and less comprehensive.
- PyTorch and Eager Execution work well with GAN and reinforcement learning, but Python doesn't work well with imitation learning.
- As a tool creator, I wanted to target applications of the future.

After talking to the autonomous driving car team at Baidu, I built my boat *deeepnav*, which runs imitation learning -- learning from the drivers.



* Autonomous Driving: DeeepNav

.image deeepnav.png _ 800



* Autonomous Driving: Imitation Learning

Supervised learning

- Learns from the driver, cannot handle exceptional/dangerous cases.

Reinforcement learning

- Learns by exhaustively exploring all possible cases in a simulator
- It is arguably more challenging to write a realistic simulator than learning to drive.

Imitation learning

- Bootstraps by supervised learning, explore near to what it knows asking for human supervisor occasionally.



* Autonomous Driving: The DAGGER' Algorithm

.code deeepnav.txt




* Autonomous Driving: Requirements


- Deeper integration of inference and training: both appear in the same driving algorithm.
- Deep learning as a function of a program, but not a separate program.
- Control flows: channels and threads for `asynchronous_update` and `communicate_with_federated_server`.
- Federated learning: collaborative learning from many drivers.
- Online learning: must allow driver engagement in exceptional cases.
- Modeling parallelism: NVIDIA DRIVE PX2 has two GPUs -- one for inference, one for model update.
- Compile-time optimization: the faster the code runs, the more real-time the control.



* Paddle: A Brief Introduction



* Paddle: Overview

- A brief history and business impact
- The open source community
- The IR, interpreter, and transpilers



* Paddle: The History

- 2012: graph-based, supports RNN, complements Caffe.
- 2013: The CEO's Award due to adoptions by Baidu products.
- 2016: Open sourced.  I designed the new generation of technology and led the development work.
- 2017: Released Paddle Fluid, the deep learning interpreter and compiler.
- 2018: 90% Baidu products turned to use Fluid, including Web search and online advertising.



* Paddle: The Community

.image community.png _ 900

Left: after opened sourcing graph-based Paddle. Right: after releasing Paddle Fluid.



* Paddle: Towards a Compiler

- IR - as [[https://github.com/PaddlePaddle/Paddle/pull/3322][protobuf messages]]: `Variable`, `Block`, `FunctionCall`, `Program`.
- [[https://github.com/PaddlePaddle/Paddle/pull/7178][Interpreter]]: executes a `Program`.
- Frontend - a Python library. Use Pythons' [[http://localhost:6060/lessons-paddle.slide#40][`with`]].
- Backend as compiler passes
- Control flows - [[https://github.com/PaddlePaddle/Paddle/issues/10244][function calls]], [[https://github.com/PaddlePaddle/Paddle/pull/6394][CSP]]
- Scope hierarchy

TensorFlow can do model/data parallelism for users.
Paddle Fluid assumes that user programs do mode/data parallelism.



* Lessons

- Model parallelism
- Distributed computing and elastic scheduling
- Inference and training are inseparable



* Model Parallelism



* Model Parallelism: Big Models

Paddle's solution to model parallelism differs from TensorFlow's.

Motivations:

- Online advertising needs terabytes big embedding layers.
- Short video classification needs NCE layers for millions of classes.





* Model Parallelism: TensorFlow

TensorFlow

- partitions the graph by the device property on nodes/edges at runtime, allowing
- `tf.device()` to assign operators to devices and
- `tf.create_partitioned_variables()` at programming time.

Our concerns:

- The programming abstraction is too low-level, e.g., exposes partitioned variables to end users.
- Device placement at compile time is too rigid and may not lead to high runtime cluster utilization.
- We don't have a sufficient number of GPUs for the many partitions needed for terabytes parameters.



* Model Parallelism: Paddle

- Outsource variable partitioning to an external storage service -- `memcached/Redis`.
- `DistributedEmbeddingLayer` and `DistributedNCELayer` access `memcached/Redis`.
- Operators like `ReadMcachedRow` and `WriteRedisRow` would allow users to define distributed layers in Python.

Following the [[https://en.wikipedia.org/wiki/Unix_philosophy][Unix philosophy]], making each program do one thing well, we out-source variable partitioning to an external storage service.



* Distributed Learning and Elastic Scheduling



* Distributed Learning: User requirements

- convergence
- efficiency

where efficiency include

- runtime efficiency: from job start to completion
- pending time: from job submission to start -- less addressed



* Distributed Learning: Elastic Scheduling

Without elastic scheduling:

- Suppose that a user submits a job requiring 10 GPUs, but the cluster has only 5 idle ones.  The job has to wait.

With elastic scheduling:

- Kubernetes starts the job with 5 processes that use the 5 GPUs.  When other jobs complete and free GPUs, increase the number of processes.

- If high-priority jobs come in, Kubernetes might kill some processes of our job to free resource.

Elastic scheduling = fault tolerance



* Distributed Learning: Synchronous SGD


- TensorFlow provides `AllReduce` for trainer processes do collaborative model update.
- No need for parameter servers.
- Each model update depends on gradients from all trainers.
- We can optimize `AllReduce` using RDMA and NVIDIA GPUDirect.
- Not fault tolerable.



* Distributed Learning: Partial Synchronous SGD

- [[https://arxiv.org/abs/1604.00981][partial synchronous SGD]] introduces a parameter server that waits for gradients from *some* of the trainers before updating the model.
- Better fault tolerance.



* Distributed Learning: Asynchronous SGD and Paddle EDL

- Fully fault-tolerable and supports [[https://kubernetes.io/blog/2017/12/paddle-paddle-fluid-elastic-learning/][elastic scheduling]]. Cluster utilization [[https://kubernetes.io/blog/2017/12/paddle-paddle-fluid-elastic-learning/][~90%]].

.image https://4.bp.blogspot.com/-gOMFfnaygSU/WiYgXO_KJ0I/AAAAAAAAAII/lMLjTGNGYhsovwKornCzMZBhEdMdPI5HACLcBGAs/s640/figure-2.png _ 500



* Distributed Learning: Federated Learning


- Google's [[https://ai.googleblog.com/2017/04/federated-learning-collaborative.html][federated learning project]] on input methods.

- Ant's requirement for federated learning is because of user privacy and data security.

- Autonomous driving's requirement is due to an unstable network connection and limited bandwidth.


Prefer deep learning in the form of functions of mobile apps in Swift or web services in C++/Go, but not a Python program running as in a separate process.



* Deeper Integration of Inference and Training


* Deeper Integration of Inference and Training: Online Learning

Internet businesses, like search, ads, and recommendation, learn user interests to improve service quality.

Online learning captures short-term interests, which leads to most clicks.

AI is part of the backend system as an RPC service:


   impression  -->    trainer  --\            /- inference
   log stream                     \          /
                      trainer  --  parameter --  inferencer   <-- online queries
   click log   -->                  server   \
   stream             trainer  --/            \- inferencer



* Deeper Integration of Inference and Training: An Example

The backend system architecture of a speech recognition startup company [[https://www.crunchbase.com/organization/unisound-beijing][Unisound]]

.image unisound-arch.png _ 900

- For good performance, not ideal for writing backend services using Python.
- Most backend developers don't use Python as their primary language.



* Deeper Integration of Inference and Training: A Modeling Perspective

Deep learning expending its applications:

- computer vision -- face recognition
- NLP -- speech recognition
- GAN -- query rewrite, keyword generation from the landing page
- reinforcement learning -- AlphaGo
- imitation learning -- autonomous driving

For the last three:

- Part of the training data comes from running inference.
- Can deploy a trained model for inference.




* Discussions: Want to Learn from Swift for TensorFlow



* Expectations

- Deeper integration of inference and training.
- Deep learning as a function, but not a program.
- Control flows including channels and threads.
- Federated learning using the edge and the cloud.
- Online learning: must allow driver engagement in exceptional cases.
- Modeling parallelism: NVIDIA DRIVE PX2 has two GPUs -- one for inference, one for model update.
- Compile-time optimization: the faster the code runs, the more real-time the control.



* Questions

Is there a plan around developer experience like syntax highlighting and IDE integration?

How is the comparison between source-to-source autodiff and generating backward code in IR?

In order to support heterogeneous computing, will you bundle GPU/ARM/x64 binary code in something like a Apple [[https://en.wikipedia.org/wiki/Universal_binary][universal binary]] file?

- `cudnn` has multiple implementations of CNN, and chooses one at runtime considering the GPU model and the filter size.





* Appendix: Paddle Details



* Paddle: Intermediate Representation

- Has *block*, function *signature*, function *call*, and [[https://github.com/PaddlePaddle/Paddle/pull/4241][*program*]].
- Need [[https://github.com/PaddlePaddle/Paddle/issues/10244][user-defined functions]].

.image ir.png _ 700


* Paddle: The Interpreter

The [[https://github.com/PaddlePaddle/Paddle/pull/4537][executor]] works like an interpreter, which runs a `Program` protobuf message.

.image interpreter.png _ 800


* Paddle: Compiler Backends as Transpilers

- For example, the [[https://github.com/wangkuiyi/Paddle/blob/8909708c1a91d580ed3c8cb12d1cab61338eac39/doc/design/fluid_compiler.md][CUDA backend]] converts `Program` messages into `.cu` files.

.image compiler.png _ 700


* Paddle: Scope Hierarchy

The compiler/interpreter needs to track variable values for the backward pass, thus cannot pop stack frames, so we generalize stacks into trees, or a [[https://github.com/PaddlePaddle/Paddle/pull/3116][scope hiearchy]].

.image scope.png _ 800


* Paddle: More Transpilers

- The [[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/transpiler/distribute_transpiler.py][distributed-training transpiler]] converts a `Program` message into a trainer program and a parameter server program.

- The [[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/transpiler/inference_transpiler.py][training-to-inference transpiler]] extracts a single iteration of inference code out from a training `Program`, so other transpilers can generate TensorFlow/Caffe graphs.

- The [[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/transpiler/memory_optimization_transpiler.py][early-free memory transpiler]] detects tensors no longer in use and inserts `FreeTensorOperators` into the program to free them early.

The [[https://en.wikipedia.org/wiki/Unix_philosophy][Unix philosophy]] -- make each program do one thing well.


* Paddle: The Frontend Language

We are too lazy to re-implement the compiler frontend for a programming language; so we slightly customize Python to generate the `Program` protobuf message.

Take [[https://github.com/PaddlePaddle/Paddle/issues/3119][IfElse]] as an example:

.image ifelse.png _ 700

