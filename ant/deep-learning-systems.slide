深度学习系统的演进和设计思路

王益
蚂蚁金服研究员
yi.w@antfin.com


* 深度学习的演进方兴未艾

.image figures/dl-gens.png 550 _


* 深度学习系统的演进方兴未艾

- 深度学习系统 是为了方便地描述 深度学习方法
- 深度学习方法的演进 推动 深度学习系统的演进
- 深度学习方兴未艾，深度学习系统的发展也在快速推进


* 最近一年里的一些大幅变化

- PyTorch 发布后，虽然它的运行速度慢，但是好用，迅速蚕食 TensorFlow 的份额。

- TensorFlow 学习 PyTorch 革命掉自己的 graph mode，去年推出 Eager Execution。预计今年底发布的 TensorFlow 2.0 将把 eager mode 设置为默认模式。使用方式接近 PyTorch。

- PyTorch 学习 TensorFlow，引入 graph，用来表示一部分计算过程 —— PyTorch 1.0 中的 script mode。

- Google Brain 去年底开发了 tangent，对深度学习计算过程的描述不同于 TensorFlow（graph）和 PyTorch（lambda chain），而是 Python 源码。后向计算的推导通过写一个 Python 编译器完成。tagent 演化成 TensorFlow AutoGraph。


* 上文中的一些关键词

- 运行速度
- 好用
- graph mode
- eager mode
- 计算的表示
- 正向计算
- 推导
- 后向计算

这些都和深度学习系统的核心功能有关


* 深度学习系统的核心功能

[[https://arxiv.org/abs/1502.05767][autodiff]] 自动求导：用户描述前向计算，系统自动推到后向计算。

- 用户懒，只想描述正向计算过程
- 训练需要执行后向计算过程
- 深度学习系统 从正向计算自动推导后向计算


* 正向计算

- 神经元网络：给定输入，通过网络，计算输出
- 预测（prediction）




* 后向计算

- 训练（training）：根据训练数据，估计参数
- 参数（parameters）：神经网络的每一个计算单元（层、operator）都可能有参数
- 后向计算：得到 ΔW = ∂c/∂W
- 优化（optimization）：更新参数 W += λ ΔW


* autodiff 过程：TensorFlow

.image figures/graph_construction_example_forward_only.png

* autodiff 过程：TensorFlow

.image figures/graph_construction_example_forward_backward.png

* autodiff 过程：TensorFlow

.image figures/graph_construction_example_all.png


* 计算过程的表示和推导

- 上文中用“图”表示计算过程是 TensorFlow 的发明；比图更直接的表达 —— 程序

四种设计思路

.image figures/toolspace.png


* 设计思路一：TensorFlow


* TensorFlow 的设计思路

- 计算过程表达成一个 graph-of-operators
- 后向计算的推导：给定表示正向计算的图，添加表示后向计算的部分
- 先构造 graph 再执行 graph
- 推导反向图的时候，尚未执行前向图，有些信息尚未知

类似思路的其他系统：Caffe2、MxNet


* TensorFlow 的例子

.code tf.py


* TensorFlow 的优势

- 通过优化图（并发执行平行的分支，融合相邻的operators）提升性能。
- 图可以作为一种结合各公司技术的规范 —— 某AI芯片能执行 TensorFlow graph。


* TensorFlow 的劣势

- 构造图的bug往往在执行的时候才能显现。
- 无法在构造图的时候发现所有问题：编译器 + 静态分析器 的工作。
- 很难实现控制流（if-else、while）


* 静态图

- 每个训练迭代执行同一个图（静态）
- 计算中往往需要包括控制流（动态）
- 让图能表示控制流：引入 IfElseOp，WhileLoopOp
- IfElseOp 位于上层图中，且有两个子图表示左右分支
- WhileLoopOp 位于上层图中，有一个子图表示 loop step

推导后向计算

- 推导的时候前向计算图尚未被执行，所以 autodiff 要考虑可能执行左/右分支、和任意轮迭代。
- 如果前向计算里嵌套 if/while，TensorFlow 至今不能正确推导后向计算：三年前报的[[https://github.com/tensorflow/tensorflow/issues/593][bug]]，一度以为已经解决了。


* TensorFlow 辅助工具

其实在解决 TensorFlow “发明”的问题

- TensorBoard
- TensorFlow debugger


能否不要发明这些问题呢？

- 放弃graph mode，转向学习 PyTorch。TensorFlow 2.0 Eager Mode。


* 设计思路二：PyTorch


* 用“程序”而不是“图”来表示计算过程

.code pytorch.py

- 不需要定义图并执行之，程序更简短
- 随处加入 print；用 Python 标准 debugger



* Lambda Chain

.image figures/chain.png 400 _


类似思路的系统：Chainer、DyNet


* 动态图

- 前向计算执行完之后，才推导后向计算
- 具备完备的信息，推导很容易

* Tape：动态度的另一种实现方式

- 执行前向计算的时候，log 每个被执行了的 operator
- 后向计算是 trace log。

类似思路的系统：TensorFlow Eager Exeuction、PaddlePaddle Tape


* 思路三：用程序表示计算过程


* 程序表示

- 图表示：执行前向计算之前，推导反向过程
- chain/tape：记录前向计算的执行过程，以便推导反向过程
- 程序表示：类似图表示

类似思路的系统：Google Tangnet、PaddlePaddle Fluid


* Google Tangent

.code tangent.py


* Googe Tangent

.image figures/tangent-autodiff.gif


* PaddlePaddle Fluid

- 不用 Python 语言描述前向后向计算，用 protobuf 定义了一种 intermediate language（IL）
- 可以有各种语言的binding，来构造 IL 程序

.code fluid.proto


* PaddlePaddle Fluid 的编译器设计

.image https://github.com/PaddlePaddle/Paddle/raw/2dc5c69ecc0d83e20101d2767a94f48e19a07ce0/doc/design/fluid-compiler.png _ 1000


* 思路四：图表示 + 执行之后推导后向计算过程


* 一种老派思路

- 出现于 Torch 2002
- 便于结合最近的图优化技术


* 图优化技术

拓扑上的优化

- TensorFlow Grappler
- NNVM
- NVIDIA TensorRT


* 四种思路的总结


* 思路一：TensorFlow、Caffe2、MxNet

- 执行图之前做 autodiff 来补全后向计算过程
- 诞生了“图优化”这种性能优化方法，被后世（？）借鉴
- 编程方式反直觉；难以调试
- 至今不支持前向计算中包括嵌套 if-else、for 的情况下 autodiff
- 因缘际会，被广泛接受；在自我革新转向第二种思路


* 思路二：PyTorch、Chainer、DyNet、PaddlePadde Tape

- 执行执行前向计算之后做 autodiff
- 编程方式自然；被广泛接受
- 前向和后向计算中部分过程可以描述成图，并通过图优化提升性能


* 思路三：Tangent、TensorFlow AutoGraph、PaddlePaddle Fluid

- 执行之前就做 autodiff —— 同思路一
- 计算过程表示为程序，而不是图 —— 同思路二
- 编程方式自然
- 通过编译技术而不是图优化提升性能 —— Google的新一代DL优化技术、NNVM2